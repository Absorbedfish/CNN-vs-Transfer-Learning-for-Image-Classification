
---

## ğŸ”¬ MetodologÃ­a

### 1ï¸âƒ£ PreparaciÃ³n de datos
- Redimensionado de imÃ¡genes a 224x224
- NormalizaciÃ³n de valores de pÃ­xeles
- Aumento de datos (rotaciÃ³n, zoom, desplazamiento, flip horizontal)
- SeparaciÃ³n entrenamiento / validaciÃ³n

---

### 2ï¸âƒ£ Modelo 1: CNN desde cero

- Capas convolucionales progresivas
- MaxPooling
- Capas densas
- Softmax para clasificaciÃ³n binaria
- Entrenamiento completo desde cero

ğŸ“‰ **Resultado:**  
El modelo aprende patrones bÃ¡sicos, pero presenta inestabilidad y limitaciones para generalizar correctamente debido al tamaÃ±o del dataset.

---

### 3ï¸âƒ£ Modelo 2: Transfer Learning (MobileNetV2)

- Modelo base preentrenado en ImageNet
- `include_top=False`
- Capas congeladas
- GlobalAveragePooling
- Capas densas ligeras
- Learning rate reducido

ğŸ“ˆ **Resultado:**  
Aprendizaje mÃ¡s rÃ¡pido, estable y con mejor capacidad de generalizaciÃ³n.

---

## ğŸ“Š Resultados

### ğŸ”¹ CNN desde cero
- Mayor fluctuaciÃ³n en la pÃ©rdida
- Menor confianza en predicciones
- Errores frecuentes en imÃ¡genes complejas

### ğŸ”¹ Transfer Learning
- PÃ©rdida muy baja
- Predicciones consistentes
- Alta confianza incluso en imÃ¡genes no vistas
- Mejor desempeÃ±o general con menos Ã©pocas

---

## ğŸ–¼ï¸ EvaluaciÃ³n visual

Se incluye una evaluaciÃ³n visual mostrando:
- Imagen original
- Clase predicha
- Probabilidad
- ComparaciÃ³n con la etiqueta real

Esto permite analizar el comportamiento real del modelo mÃ¡s allÃ¡ de mÃ©tricas numÃ©ricas.

---

## âœ… Conclusiones

- El Transfer Learning es significativamente mÃ¡s eficiente que entrenar una CNN desde cero para datasets pequeÃ±os.
- MobileNetV2 permite aprovechar conocimiento previo aprendido en grandes conjuntos de datos.
- Ajustar correctamente la arquitectura y la tasa de aprendizaje es clave para evitar sobreconfianza.
- La evaluaciÃ³n visual es fundamental para entender el comportamiento del modelo.

---

## ğŸš€ Posibles mejoras futuras

- Fine-tuning parcial de las capas finales del modelo base
- Uso de callbacks como EarlyStopping
- AmpliaciÃ³n del dataset
- InclusiÃ³n de mÃ¡s clases

---

## ğŸ‘¤ Autor

Proyecto desarrollado por **Manuel Antonio Loaiza Pintor**  
Estudiante autodidacta de Machine Learning y Computer Vision.
